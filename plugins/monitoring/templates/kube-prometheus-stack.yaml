# https://github.com/prometheus-operator/prometheus-operator/tree/main/helm
# https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter
# https://artifacthub.io/packages/helm/banzaicloud-stable/grafana-operator
# https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics
apiVersion: plugins.kubegems.io/v1beta1
kind: Plugin
metadata:
  name: kube-prometheus-stack
  namespace: {{ .Release.Namespace }}
spec:
  kind: helm
  url: https://prometheus-community.github.io/helm-charts
  version: {{ .Chart.AppVersion }}
  values:
    global:
      rbac:
        createAggregateClusterRoles: true
    defaultRules:
      create: false
    kube-state-metrics:
      image:
        # repository: k8s.gcr.io/kube-state-metrics/kube-state-metrics
        {{ include "common.images.repository" ( dict "registry" "k8s.gcr.io" "repository" "kube-state-metrics/kube-state-metrics" "context" .) }}
      metricLabelsAllowlist:
        - namespaces=[*] # scrape namespace labels
      prometheus:
        monitor:
          metricRelabelings:
            # drop this becase it has label service_name, which affect otel dashboard variables
            - sourceLabels: [__name__]
              regex: kube_ingress_path
              action: drop
    grafana:
      enabled: false
    prometheusOperator:
      tls:
        enabled: false
      admissionWebhooks:
        enabled: false
      image:
        # repository: quay.io/prometheus-operator/prometheus-operator
        {{ include "common.images.repository" ( dict "registry" "quay.io" "repository" "prometheus-operator/prometheus-operator" "context" .) }}
        # tag: v0.50.1-gems
      prometheusConfigReloader:
        image:
          # repository: quay.io/prometheus-operator/prometheus-config-reloader
          {{ include "common.images.repository" ( dict "registry" "quay.io" "repository" "prometheus-operator/prometheus-config-reloader" "context" .) }}
          # tag: v0.50.1-gems
    prometheus:
      prometheusSpec:
        image:
          # repository: quay.io/prometheus/prometheus
          {{ include "common.images.repository" ( dict "registry" "quay.io" "repository" "prometheus/prometheus" "context" .) }}
        enableAdminAPI: true
        enableFeatures: 
          - remote-write-receiver
        externalLabels:
          cluster: {{ .Values.global.clusterName }}
        # TODO: use correct port when we can get default-gateway's nodeport
        externalUrl: http://prometheus.{{ .Values.global.clusterName }}.kubegems.io:9090
        ruleSelector:
          matchExpressions:
            - key: prometheusrule.kubegems.io/name
              operator: Exists
        probeSelectorNilUsesHelmValues: false
        retention: 30d
        resources:
          limits:
            cpu: 4
            memory: 8Gi
          requests:
            cpu: 2
            memory: 4Gi
        storageSpec:
          volumeClaimTemplate:
            spec:
              accessModes:
              - ReadWriteOnce
              storageClassName: {{ .Values.global.storageClass }}
              resources:
                requests:
                  storage: 200Gi
              volumeMode: Filesystem
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
      additionalServiceMonitors:
        - name: kubegems-local-agent
          namespaceSelector:
            matchNames:
              - kubegems-local
          selector:
            matchLabels:
              app.kubernetes.io/name: kubegems-local
              app.kubernetes.io/component: agent
          endpoints:
            - path: /metrics
              port: metrics
              honorLabels: true
        - name: cert-manager
          namespaceSelector:
            matchNames:
              - cert-manager
          selector:
            matchLabels:
              app.kubernetes.io/name: cert-manager
              app.kubernetes.io/component: controller
          endpoints:
            - path: /metrics
              port: tcp-prometheus-servicemonitor
              honorLabels: true
      ingress:
        enabled: true
        ingressClassName: default-gateway
        labels:
          networking.kubegems.io/ingressClass: default-gateway        
        hosts: [prometheus.{{ .Values.global.clusterName }}.kubegems.io]
    alertmanager:
      # templateFiles:
      alertmanagerSpec:
        image:
          # repository: quay.io/prometheus/alertmanager
          {{ include "common.images.repository" ( dict "registry" "quay.io" "repository" "prometheus/alertmanager" "context" .) }}
        alertmanagerConfigSelector:
          matchExpressions:
            - key: alertmanagerconfig.kubegems.io/name
              operator: Exists
        alertmanagerConfiguration:
          name: kubegems-default-monitor-alert-rule
          templates:
          - configMap:
              key: kubegems-email.tmpl
              name: kubegems-email-template
        retention: 240h
        storage:
          volumeClaimTemplate:
            spec:
              accessModes:
              - ReadWriteOnce
              storageClassName: {{ .Values.global.storageClass }}
              resources:
                requests:
                  storage: 10Gi
              volumeMode: Filesystem
        # TODO: use correct port when we can get default-gateway's nodeport
        externalUrl: http://alertmanager.{{ .Values.global.clusterName }}.kubegems.io:9093
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 128Mi
      ingress:
        enabled: true
        ingressClassName: default-gateway
        labels:
          networking.kubegems.io/ingressClass: default-gateway
        hosts: [alertmanager.{{ .Values.global.clusterName }}.kubegems.io]
    nodeExporter:
      enabled: false
    # metrics
    kubeApiServer:
      serviceMonitor:
        metricRelabelings:
          - sourceLabels: [__name__]
            regex: apiserver_(admission|crd|watch_events)_(.*)
            action: drop